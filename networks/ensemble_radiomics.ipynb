{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066974bf-644c-4e0a-924d-b4ed0051dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/m324371/PyEnv/adnexal/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author@ Mrinal Kanti Dhar\n",
    "October 30, 2024\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/research/m324371/Project/adnexal/utils/\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from classification_head import ClassificationHeadWithoutFlatten\n",
    "from feature_ensemble_2models import FeatureEnsemble2models\n",
    "import attention\n",
    "\n",
    "from ensemble_type1 import EnsembleResNet18Ft512_EfficientNetB2SFt1408V2\n",
    "from radiomic_nets import RadiomicMLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cc880-ef8e-47ce-a9a1-8064a279ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiomicMLP_EnsembleResNet18Ft512_EfficientNetB2SFt1408(nn.Module):\n",
    "    \"\"\" Ensembles Radiomic features from RadiomicMLP with EnsembleResNet18Ft512_EfficientNetB2SFt1408 \n",
    "        EnsembleResNet18Ft512_EfficientNetB2SFt1408 ensembles ResNet18 with 512 features and EfficientNetB2 with 1408 features\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes:int,\n",
    "                 out_channels:list=None,  # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain:bool=True,\n",
    "                 dropout:float=0.3,\n",
    "                 in_chs:int=None,\n",
    "                 separate_inputs:int=None, # separate_inputs defines the number of inputs\n",
    "\n",
    "                 radiomic_dims:list=None, \n",
    "                 radiomic_activation:str='leakyrelu', \n",
    "                 radiomic_attention=None, \n",
    "                 radiomic_dropout:float=None,\n",
    "                ):  \n",
    "        super(RadiomicMLP_EnsembleResNet18Ft512_EfficientNetB2SFt1408, self).__init__()\n",
    "\n",
    "        \"Prepare deep learning model (up to feature extraction)\"\n",
    "        # Initialize deep learning model\n",
    "        self.dl_model = EnsembleResNet18Ft512_EfficientNetB2SFt1408V2(num_classes, out_channels, pretrain, dropout, in_chs, separate_inputs, only_feature_extraction=True) \n",
    "        # Trim classification head\n",
    "        # self.dl_feature_extractor = nn.Sequential(*list(dl_model.children()))[:-1] \n",
    "\n",
    "        # modules = []\n",
    "        # for layer in dl_model.children():\n",
    "        #     if isinstance(layer, nn.ModuleList):\n",
    "        #         modules.extend(layer)  # Flatten out ModuleList layers\n",
    "        #     else:\n",
    "        #         modules.append(layer)\n",
    "        # self.dl_feature_extractor = nn.Sequential(*modules[:-1])  # Exclude the classification head\n",
    "\n",
    "\n",
    "        # Global average pooling\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Output size: (out_channels x 1 x 1)\n",
    "        \n",
    "        # Find no. of output features using dummy input\n",
    "        dummy_inp = torch.rand(1, in_chs, 224, 224)\n",
    "        dummy_out = self.dl_model(dummy_inp)\n",
    "        dl_feature_dim = dummy_out.shape[1] # no. of channels\n",
    "        \n",
    "        \"Prepare radiomic model\"\n",
    "        # Get attention module\n",
    "        if radiomic_attention is not None: radiomic_attention = getattr(attention, radiomic_attention) \n",
    "        self.radiomic_model = RadiomicMLP(radiomic_dims, radiomic_activation, radiomic_attention, radiomic_dropout)\n",
    "\n",
    "        \"Prepare classification head\"\n",
    "        # The input features of the classification head is dl_feature_dim + radiomic_out_dim\n",
    "        in_features_for_cl = dl_feature_dim + radiomic_dims[-1]\n",
    "        cl_dims = [in_features_for_cl] + out_channels # classification dimensions\n",
    "        \n",
    "        # We assume that the features are already flattened\n",
    "        self.classification = ClassificationHeadWithoutFlatten(num_classes=num_classes,\n",
    "                                                 out_channels=cl_dims,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, data_img, data_radiomic):\n",
    "        # Extract features\n",
    "        dl_features = self.dl_model(data_img)\n",
    "        dl_features = self.avg_pool(dl_features)\n",
    "        dl_features = dl_features.view(dl_features.size(0), -1) # Shape: (Batch_size, features)\n",
    "\n",
    "        radiomic_features = self.radiomic_model(data_radiomic) # Shape: (batch_size * num_features, mlp_output_dim)\n",
    "        radiomic_features = radiomic_features.view(data_radiomic.size(0), -1) # Shape: (Batch_size, features)\n",
    "        \n",
    "        # Concatenate DL and radiomic features\n",
    "        combined_features = torch.cat((dl_features, radiomic_features), dim=1)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classification(combined_features)\n",
    "\n",
    "        return output\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3b2e0-5f0c-4420-a868-0379fb892826",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_inpput = torch.rand(4, 3, 224, 224)\n",
    "    num_classes = 2\n",
    "    out_channels = [5376, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout = 0.3\n",
    "    separate_inputs = 3\n",
    "    in_chs = 3\n",
    "    \n",
    "    radiomic_input = torch.randn(4, 1, 10)\n",
    "    radiomic_dims = [10, 64, 128, 256, 512]\n",
    "    radiomic_activation = 'leakyrelu'\n",
    "    radiomic_dropout = 0.3\n",
    "    radiomic_attention = \"BasicAttention\" # define the attention module to be used\n",
    "    \n",
    "    model = RadiomicMLP_EnsembleResNet18Ft512_EfficientNetB2SFt1408(num_classes, out_channels, pretrain, dropout, in_chs, separate_inputs,\n",
    "                                                                    radiomic_dims, radiomic_activation, \n",
    "                                                                    radiomic_attention, radiomic_dropout)\n",
    "    \n",
    "    out = model(image_inpput, radiomic_input)\n",
    "    print(out.shape) # torch.Size([4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b65b8-e4fe-493b-b9fd-1805bf724d07",
   "metadata": {},
   "source": [
    "### RadiomicMLP_EnsembleResNet18Ft512_EfficientNetB2SFt1408V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a89e66-be0a-45f3-a9b9-4ee0e5781ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiomicMLP_EnsembleResNet18Ft512_EfficientNetB2SFt1408V2(nn.Module):\n",
    "    \"\"\" Ensembles Radiomic features from RadiomicMLP with EnsembleResNet18Ft512_EfficientNetB2SFt1408 \n",
    "        EnsembleResNet18Ft512_EfficientNetB2SFt1408 ensembles ResNet18 with 512 features and EfficientNetB2 with 1408 features\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes:int,\n",
    "                 out_channels:list=None,  # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain:bool=True,\n",
    "                 dropout:float=0.3,\n",
    "                 in_chs:int=None,\n",
    "                 separate_inputs:int=None, # separate_inputs defines the number of inputs\n",
    "\n",
    "                 radiomic_dims:list=None, \n",
    "                 radiomic_activation:str='leakyrelu', \n",
    "                 radiomic_attention=None, \n",
    "                 radiomic_dropout:float=None,\n",
    "\n",
    "                 ft_normalization:list=[None, None], # either None, \"in\", \"ln\", or \"bn\". \n",
    "                 # The 1st normalization is used to normalize the final feature map. The 2nd normalization is used to normalize\n",
    "                 # the concatenated DL and radiomic flattened features. So, don't use instance normalization for the 2nd normalization.\n",
    "                 cls_activation:str='leakyrelu', \n",
    "                ):  \n",
    "        super(RadiomicMLP_EnsembleResNet18Ft512_EfficientNetB2SFt1408V2, self).__init__()\n",
    "\n",
    "        \"Prepare deep learning model (up to feature extraction)\"\n",
    "        # Initialize deep learning model\n",
    "        self.dl_model = EnsembleResNet18Ft512_EfficientNetB2SFt1408V2(num_classes, out_channels, pretrain, dropout, in_chs, separate_inputs, only_feature_extraction=True) \n",
    "        # Trim classification head\n",
    "        # self.dl_feature_extractor = nn.Sequential(*list(dl_model.children()))[:-1] \n",
    "\n",
    "        # modules = []\n",
    "        # for layer in dl_model.children():\n",
    "        #     if isinstance(layer, nn.ModuleList):\n",
    "        #         modules.extend(layer)  # Flatten out ModuleList layers\n",
    "        #     else:\n",
    "        #         modules.append(layer)\n",
    "        # self.dl_feature_extractor = nn.Sequential(*modules[:-1])  # Exclude the classification head\n",
    "        \n",
    "        \n",
    "        # Global average pooling\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Output size: (batch, out_channels x 1 x 1)\n",
    "        \n",
    "        # Find no. of output features using dummy input\n",
    "        dummy_inp = torch.rand(1, in_chs, 224, 224)\n",
    "        dummy_out = self.dl_model(dummy_inp) # shape: batch x ch x h x w\n",
    "        dl_feature_dim = dummy_out.shape[1] # no. of channels\n",
    "\n",
    "        \"Prepare radiomic model\"\n",
    "        # Get attention module\n",
    "        if radiomic_attention is not None: radiomic_attention = getattr(attention, radiomic_attention) \n",
    "        self.radiomic_model = RadiomicMLP(radiomic_dims, radiomic_activation, radiomic_attention, radiomic_dropout)\n",
    "\n",
    "        \"Prepare classification head\"\n",
    "        # The input features of the classification head is dl_feature_dim + radiomic_out_dim\n",
    "        in_features_for_cl = dummy_out.shape[1] + radiomic_dims[-1]\n",
    "        cl_dims = [in_features_for_cl] + out_channels # classification dimensions \n",
    "        \n",
    "        # We assume that the features are already flattened\n",
    "        self.classification = ClassificationHeadWithoutFlatten(num_classes=num_classes,\n",
    "                                                 out_channels=cl_dims,\n",
    "                                                 activation=cls_activation,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "        \"Prepare normalization - one to normalize DL features, and the other to normalize combined features\"\n",
    "        # Normalize DL features \n",
    "        if ft_normalization[0] == None:\n",
    "            self.norm1 = None\n",
    "        elif ft_normalization[0] == \"in\":\n",
    "            self.norm1 = nn.InstanceNorm2d(dummy_out.shape[1], affine=True) # affine adds learnable scale/shift\n",
    "        elif ft_normalization[0] == \"ln\":\n",
    "            self.norm1 = nn.LayerNorm([dummy_out.shape[1],dummy_out.shape[2],dummy_out.shape[3]]) # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< PAY ATTENTION !!!! If separate inputs\n",
    "        elif ft_normalization[0] == \"bn\":\n",
    "            self.norm1 = nn.BatchNorm2d(num_features=dummy_out.shape[1])\n",
    "        else:\n",
    "            raise ValueError(\"Wrong keyword for normalization. Permitted keywords are - None, in, ln, and bn.\")    \n",
    "\n",
    "        # Normalize combined features \n",
    "        if ft_normalization[1] == None:\n",
    "            self.norm2 = None\n",
    "        elif ft_normalization[1] == \"ln\":\n",
    "            self.norm2 = nn.LayerNorm(in_features_for_cl) # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< PAY ATTENTION !!!! If separate inputs\n",
    "        elif ft_normalization[1] == \"bn\":\n",
    "            self.norm2 = nn.BatchNorm1d(num_features=in_features_for_cl) \n",
    "        else:\n",
    "            raise ValueError(\"Wrong keyword for normalization. Permitted keywords are - None, ln, and bn.\")\n",
    "\n",
    "    def forward(self, data_img, data_radiomic):\n",
    "        # Extract features\n",
    "        dl_features = self.dl_model(data_img)\n",
    "        if self.norm1 is not None: dl_features = self.norm1(dl_features)  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< PAY ATTENTION !!!!\n",
    "        dl_features = self.avg_pool(dl_features) # Output size: (batch, out_channels x 1 x 1)\n",
    "        dl_features = dl_features.view(dl_features.size(0), -1) # Shape: (Batch_size, features)\n",
    "\n",
    "        radiomic_features = self.radiomic_model(data_radiomic) # Shape: (batch_size * num_features, mlp_output_dim)\n",
    "        radiomic_features = radiomic_features.view(data_radiomic.size(0), -1) # Shape: (Batch_size, features)\n",
    "        \n",
    "        # Concatenate DL and radiomic features\n",
    "        combined_features = torch.cat((dl_features, radiomic_features), dim=1)\n",
    "        if self.norm2 is not None: combined_features = self.norm2(combined_features)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classification(combined_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec901ac-ec90-4910-ae86-4b0c67b92c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_inpput = torch.rand(4, 3, 224, 224)\n",
    "    num_classes = 2\n",
    "    out_channels = [5376, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout = 0.3\n",
    "    separate_inputs = 3\n",
    "    in_chs = 3\n",
    "    \n",
    "    radiomic_input = torch.randn(4, 1, 10)\n",
    "    radiomic_dims = [10, 64, 128, 256, 512]\n",
    "    radiomic_activation = 'leakyrelu'\n",
    "    radiomic_dropout = 0.3\n",
    "    radiomic_attention = \"BasicAttention\" # define the attention module to be used\n",
    "    \n",
    "    ft_normalization = [\"in\", \"bn\"] # either None, \"ln\", or \"bn\"\n",
    "    cls_activation = \"leakyrelu\"\n",
    "    \n",
    "    model = RadiomicMLP_EnsembleResNet18Ft512_EfficientNetB2SFt1408V2(num_classes, out_channels, pretrain, dropout, in_chs, separate_inputs,\n",
    "                                                                    radiomic_dims, radiomic_activation, radiomic_attention, radiomic_dropout,\n",
    "                                                                   ft_normalization, cls_activation)\n",
    "    \n",
    "    out = model(image_inpput, radiomic_input)\n",
    "    print(out.shape) # torch.Size([4, 2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908bb99-47f7-44b7-9805-3ac6fa6a5a44",
   "metadata": {},
   "source": [
    "### RadiomicMLP_AlreadyTrainedEnsembleResNet18Ft512_EfficientNetB2SFt1408V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f56e44-4e40-41a5-aecd-d2e3e25657e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/research/m324371/Project/adnexal/\")\n",
    "\n",
    "import os\n",
    "from networks import nets\n",
    "\n",
    "#%% Parameters\n",
    "BASE_MODEL = \"EnsembleResNet18Ft512_EfficientNetB2SFt1408\" # config.model.name\n",
    "\n",
    "# base_model_name = \"EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-29_11-39-58\" # config.test.base_model_name\n",
    "base_model_name = \"EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_17-31-23\" # config.test.base_model_name\n",
    "\n",
    "# Result directory\n",
    "result_dir = \"/research/m324371/Project/adnexal/results/\" # config.directories.result_dir\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LR = 0.001 # config.train.lr #0.0001 # learning rate\n",
    "WEIGHT_DECAY = 0.001 # config.train.weight_decay #1e-5\n",
    "\n",
    "BATCH_SIZE = 1 # config.train.batch_size\n",
    "ONE_HOT = True # config.train.one_hot\n",
    "N_CLASSES = 2 # config.train.n_classes\n",
    "ONLY_ADNEXAL = False # config.data.only_adnexal\n",
    "ONLY_FLUID = True # config.data.only_fluid\n",
    "ONLY_SOLID = True # config.data.only_solid\n",
    "DRAW_BBOX = False # config.data.draw_bbox\n",
    "CROP_ROI = True # config.data.crop_roi\n",
    "MARGIN = 200 # config.data.margin\n",
    "RESIZE = True # config.data.resize\n",
    "KEEP_ASPECT_RATIO = True # config.data.keep_aspect_ratio\n",
    "TARGET_SIZE = [224,224] # config.data.target_size\n",
    "CONCAT = [\"image\", \"fluid\", \"solid\"] # config.data.concat # Possible keywords are: \"image\", \"adnexal\", \"fluid\", \"solid\", \"mask\"\n",
    "INPUT_CH = len(CONCAT)\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Parameters for ensemble models\n",
    "DROPOUT = 0.3 # config.model.dropout\n",
    "OUT_CHS = [5376, 1024, 512, 256] # config.model.out_channels # concat feature maps will be converted to OUT_CHS\n",
    "\n",
    "\n",
    "\n",
    "get_model = getattr(nets, BASE_MODEL) # get_model is a model class, not an object\n",
    "params = dict()\n",
    "params[\"num_classes\"] = N_CLASSES\n",
    "params[\"out_channels\"] = OUT_CHS\n",
    "params[\"pretrain\"] = True\n",
    "params[\"dropout\"] = DROPOUT \n",
    "params[\"in_chs\"] = len(CONCAT) # len(config.data.concat) # <<<<<<<<<<<<<<<<<<<<<<<<<<<<< temporarily changed for doppler\n",
    "params[\"separate_inputs\"] = 3 # config.model.separate_inputs\n",
    "\n",
    "\n",
    "base_model = get_model(**params)\n",
    "# base_model.classification.fc = nn.Identity() # keep till flattening \n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_17-31-37\",\n",
    "    \"EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_17-58-11\",\n",
    "    \"EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_18-21-46\",\n",
    "    \"EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_18-47-41\",\n",
    "    \"EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_19-15-38\",\n",
    "]\n",
    "\n",
    "\n",
    "# Collect all trained models\n",
    "trained_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    checkpoint_loc = os.path.join(result_dir, base_model_name, 'checkpoints', model_name)\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_loc, 'best_model.pth'))\n",
    "\n",
    "    # Make a deep copy of the base model\n",
    "    model_copy = deepcopy(base_model)\n",
    "\n",
    "    # Load the weights into the copied model\n",
    "    model_copy.load_state_dict(checkpoint['state_dict'])\n",
    "    model_copy.eval()  # Set the copied model to evaluation mode\n",
    "\n",
    "    # Attach hook to capture features from an intermediate layer\n",
    "    layer_to_hook = model_copy.classification.avg_pool # layer to hook <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    layer_to_hook.register_forward_hook(hook)\n",
    "\n",
    "    # Append the copied model to the list of trained models\n",
    "    trained_models.append(model_copy.to(DEVICE))\n",
    "\n",
    "print(f\"No. of models: {len(trained_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8311102-1e68-4879-b2fa-db2fca3edfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10193d-2ee7-46cd-8f6a-285b12d4fa28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa2390f1-282c-4019-961a-e81e5f566565",
   "metadata": {},
   "source": [
    "### RadiomicMLP_PretrainedEnsembleResNet18Ft512_EfficientNetB2SFt1408V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e9c25a-0e6a-4182-adea-7ce210bd8370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/research/m324371/Project/adnexal/networks/\") \n",
    "from ensemble_type1 import EnsembleResNet18Ft512_EfficientNetB2SFt1408\n",
    "\n",
    "class RadiomicMLP_PretrainedEnsembleResNet18Ft512_EfficientNetB2SFt1408(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes:int,\n",
    "                 out_channels:list=None,\n",
    "                 pretrain:bool=True,\n",
    "                 pretrained_dir:str=None,  # Path to pretrained model\n",
    "                 dropout:float=0.3,\n",
    "                 in_chs:int=None,\n",
    "                 separate_inputs:int=None,\n",
    "                 radiomic_dims:list=None, \n",
    "                 radiomic_activation:str='leakyrelu', \n",
    "                 radiomic_attention=None, \n",
    "                 radiomic_dropout:float=None,\n",
    "                 ft_normalization:list=[None, None],\n",
    "                 cls_activation:str='leakyrelu'):  \n",
    "        super(RadiomicMLP_PretrainedEnsembleResNet18Ft512_EfficientNetB2SFt1408, self).__init__()\n",
    "\n",
    "        # Initialize the deep learning model\n",
    "        self.dl_model = EnsembleResNet18Ft512_EfficientNetB2SFt1408(num_classes, \n",
    "                                                                      out_channels, \n",
    "                                                                      pretrain, \n",
    "                                                                      dropout, \n",
    "                                                                      in_chs, \n",
    "                                                                      separate_inputs,)\n",
    "        \n",
    "        # Load pretrained parameters if provided\n",
    "        if pretrained_dir:\n",
    "            print(f\"Loading pretrained model from: {pretrained_dir}\")\n",
    "            checkpoint = torch.load(pretrained_dir)\n",
    "            # state_dict = torch.load(pretrained_dir, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            self.dl_model.load_state_dict(checkpoint['state_dict'])\n",
    "            # self.dl_model.load_state_dict(state_dict)\n",
    "        \n",
    "        # Freeze the pretrained model parameters\n",
    "        for param in self.dl_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove classification layers and keep till flattening layer\n",
    "        self.dl_model.classification.fc = nn.Identity() # keep till flattening \n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.dl_model.eval()\n",
    "\n",
    "        # Global average pooling\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Output size: (batch, out_channels x 1 x 1)\n",
    "        \n",
    "        # Prepare radiomic model\n",
    "        if radiomic_attention is not None:\n",
    "            radiomic_attention = getattr(attention, radiomic_attention)\n",
    "        self.radiomic_model = RadiomicMLP(radiomic_dims, radiomic_activation, radiomic_attention, radiomic_dropout)\n",
    "\n",
    "        # Classification head\n",
    "        dummy_inp = torch.rand(1, in_chs, 224, 224)\n",
    "        dummy_out = self.dl_model(dummy_inp) # torch.Size([1, 5376])\n",
    "        dl_feature_dim = dummy_out.shape[1]\n",
    "        in_features_for_cl = dl_feature_dim + radiomic_dims[-1]\n",
    "        cl_dims = [in_features_for_cl] + out_channels\n",
    "        \n",
    "        self.classification = ClassificationHeadWithoutFlatten(num_classes=num_classes,\n",
    "                                                               out_channels=cl_dims,\n",
    "                                                               activation=cls_activation,\n",
    "                                                               dropout=dropout)\n",
    "\n",
    "        # Normalization layers\n",
    "        self.norm1 = self._get_normalization(ft_normalization[0], dummy_out.shape)\n",
    "        self.norm2 = self._get_normalization(ft_normalization[1], (in_features_for_cl,))\n",
    "    \n",
    "    def _get_normalization(self, norm_type, shape):\n",
    "        if norm_type is None:\n",
    "            return None\n",
    "        elif norm_type == \"in\":\n",
    "            return nn.InstanceNorm2d(shape[1], affine=True)\n",
    "        elif norm_type == \"ln\":\n",
    "            return nn.LayerNorm(shape)\n",
    "        elif norm_type == \"bn\":\n",
    "            return nn.BatchNorm2d(shape[1]) if len(shape) == 4 else nn.BatchNorm1d(shape[0])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid normalization type.\")\n",
    "    \n",
    "    def forward(self, data_img, data_radiomic):\n",
    "        # Extract features\n",
    "        with torch.no_grad():  # Ensure pretrained model is frozen\n",
    "            dl_features = self.dl_model(data_img)\n",
    "            if self.norm1 is not None:\n",
    "                dl_features = self.norm1(dl_features)\n",
    "            # dl_features = self.avg_pool(dl_features)\n",
    "            # dl_features = dl_features.view(dl_features.size(0), -1)\n",
    "\n",
    "        # Radiomic features\n",
    "        radiomic_features = self.radiomic_model(data_radiomic)\n",
    "        radiomic_features = radiomic_features.view(data_radiomic.size(0), -1)\n",
    "        \n",
    "        # Concatenate DL and radiomic features\n",
    "        combined_features = torch.cat((dl_features, radiomic_features), dim=1)\n",
    "        if self.norm2 is not None:\n",
    "            combined_features = self.norm2(combined_features)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classification(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ba81d1d-f307-46c5-98a6-3cec3dc51cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from: /research/m324371/Project/adnexal/results/EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_17-31-23/checkpoints/EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_19-15-38/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2609089/1943525478.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_inpput = torch.rand(4, 3, 224, 224)\n",
    "    num_classes = 2\n",
    "    out_channels = [5376, 1024, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout = 0.3\n",
    "    separate_inputs = 3\n",
    "    in_chs = 3\n",
    "    pretrained_dir='/research/m324371/Project/adnexal/results/EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_17-31-23/checkpoints/EnsembleResNet18Ft512_EfficientNetB2SFt1408_2024-10-25_19-15-38/best_model.pth'\n",
    "    \n",
    "    radiomic_input = torch.randn(4, 1, 10)\n",
    "    radiomic_dims = [10, 64, 128, 256, 512]\n",
    "    radiomic_activation = 'leakyrelu'\n",
    "    radiomic_dropout = 0.3\n",
    "    radiomic_attention = \"BasicAttention\" # define the attention module to be used\n",
    "    \n",
    "    ft_normalization = [None, \"ln\"] # either None, \"ln\", or \"bn\"\n",
    "    cls_activation = \"leakyrelu\"\n",
    "    \n",
    "    model = RadiomicMLP_PretrainedEnsembleResNet18Ft512_EfficientNetB2SFt1408(num_classes, out_channels, pretrain, pretrained_dir, dropout, in_chs, separate_inputs,\n",
    "                                                                    radiomic_dims, radiomic_activation, radiomic_attention, radiomic_dropout,\n",
    "                                                                   ft_normalization, cls_activation)\n",
    "    \n",
    "    out = model(image_inpput, radiomic_input)\n",
    "    print(out.shape) # torch.Size([4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f83b-ec91-437c-b557-a388cc4edfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
