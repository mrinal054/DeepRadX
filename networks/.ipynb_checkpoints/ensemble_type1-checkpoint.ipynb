{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066974bf-644c-4e0a-924d-b4ed0051dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/m324371/PyEnv/adnexal/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author@ Mrinal Kanti Dhar\n",
    "October 24, 2024\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/research/m324371/Project/adnexal/utils/\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "\n",
    "from pscse_cab import PscSEWithCAB\n",
    "from classification_head import ClassificationHead\n",
    "from feature_ensemble_2models import FeatureEnsemble2models\n",
    "from feature_ensemble import FeatureEnsembleNModelsNoTrim\n",
    "\n",
    "from res50pscse_512x28x28 import ResNet50Pscse_512x28x28\n",
    "from enetb2lpscse_384x28x28 import EfficientNetB2LPscse_384x28x28\n",
    "\n",
    "from base_models_collection import base_models\n",
    "from base_models_features_collection import base_models_features_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bd969-0f54-42e8-a239-f387d8fbc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureEnsembleNModelsNoTrim(nn.Module):\n",
    "#     \"\"\"Ensemble feature maps of N models. Assumes the classification heads\n",
    "#     are already removed from the models, so no trimming operation is done here.\n",
    "\n",
    "#     Args:\n",
    "#         - models: A list of models to be ensembled\n",
    "\n",
    "#     Returns:\n",
    "#         - combined_features: A feature map combining all input models\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, models):\n",
    "#         super(FeatureEnsembleNModelsNoTrim, self).__init__()\n",
    "        \n",
    "#         self.models = nn.ModuleList(models)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features_list = []\n",
    "        \n",
    "#         # Extract features from each model\n",
    "#         for model in self.models:\n",
    "#             features = model(x)\n",
    "#             features_list.append(features)\n",
    "        \n",
    "#         if len(features_list) > 1:\n",
    "#             # Resize feature maps if needed to match the spatial dimensions of the first model's output\n",
    "#             base_shape = features_list[0].shape[2:]\n",
    "#             for i in range(1, len(features_list)):\n",
    "#                 if features_list[i].shape[2:] != base_shape:\n",
    "#                     features_list[i] = F.interpolate(features_list[i], size=base_shape, mode='bilinear', align_corners=False)\n",
    "\n",
    "#         # Concatenate the feature maps along the channel dimension\n",
    "#         combined_features = torch.cat(features_list, dim=1)\n",
    "\n",
    "#         return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c329a2b-ee21-4265-8d90-877dc03bce49",
   "metadata": {},
   "source": [
    "### EnsembleResNet18Ft512_EfficientNetB2SFt1408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2879b8-f8a4-46ac-b2db-2b7e58f03393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleResNet18Ft512_EfficientNetB2SFt1408(nn.Module):\n",
    "    \"\"\" Ensembles ResNet18 with 512 features and EfficientNetB2 with 1408 features \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes: int,\n",
    "                 out_channels: list = None,  # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain: bool = True,\n",
    "                 dropout: float = 0.3,\n",
    "                 in_chs: int = None,\n",
    "                 separate_inputs: int = None):  # separate_inputs defines the number of inputs\n",
    "\n",
    "        super(EnsembleResNet18Ft512_EfficientNetB2SFt1408, self).__init__()\n",
    "\n",
    "        self.separate_inputs = separate_inputs\n",
    "\n",
    "        model1 = base_models('resnet18', pretrain=pretrain, num_classes=num_classes, in_chs=in_chs)\n",
    "        model2 = base_models('efficientnet_v2_s', pretrain=pretrain, num_classes=num_classes, in_chs=in_chs)\n",
    "\n",
    "        self.ens_model1 = FeatureEnsemble2models(model1, model2, trim1=2, trim2=2)  # clip classification head\n",
    "\n",
    "        # Create a list of models for separate inputs if separate_inputs is specified\n",
    "        if self.separate_inputs is not None:\n",
    "            self.ensemble_models = nn.ModuleList([deepcopy(self.ens_model1) for _ in range(self.separate_inputs)])\n",
    "\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.separate_inputs is not None:\n",
    "\n",
    "            # Ensure input data has self.separate_inputs no. of channels\n",
    "            if x.shape[1] < self.separate_inputs:\n",
    "                raise ValueError(f\"Can't split. Input data has {x.shape[1]} channels whereas separate_inputs parameter is {self.separate_inputs}. \\\n",
    "Check the separate_inputs parameter in the config file.\")\n",
    "            \n",
    "            features_list = []\n",
    "\n",
    "            # Loop over each input channel, process it, and store the features\n",
    "            for i in range(self.separate_inputs):\n",
    "                # Separate the i-th input (single channel)\n",
    "                xi = x[:, i:i + 1, :, :]  # extract ith channel\n",
    "\n",
    "                # Convert to 3 channels by repeating or concatenating along the channel dimension\n",
    "                xi_3ch = torch.cat([xi, xi, xi], dim=1)\n",
    "\n",
    "                # Get features from the i-th ensemble model\n",
    "                features_i = self.ensemble_models[i](xi_3ch)\n",
    "\n",
    "                # Collect features\n",
    "                features_list.append(features_i)\n",
    "\n",
    "            # Concatenate features along the channel dimension\n",
    "            features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        else:\n",
    "            features = self.ens_model1(x)\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544acc1f-c92b-413f-9cc4-c0d0c92f86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    inp=torch.rand(1, 3, 224, 224)\n",
    "    num_classes=2\n",
    "    out_channels=[5376, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout=0.3\n",
    "    separate_inputs = 3\n",
    "    in_channels = 3\n",
    "    \n",
    "    model = EnsembleResNet18Ft512_EfficientNetB2SFt1408(num_classes, out_channels, pretrain, dropout, in_channels, separate_inputs)\n",
    "    \n",
    "    out = model(inp)\n",
    "    \n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678ef7c-d4f1-48d3-aab9-465c3b84decc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768700c-8bbc-43a2-a67f-91f9971fe9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4a0c9f-d6ba-4c60-a5ee-4b5077a5597a",
   "metadata": {},
   "source": [
    "### EnsembleResNet50_512x28PscseEfficientNetB2Pscse384X28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee99f0-77a9-4123-8b69-d449436fefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleResNet50_512x28PscseEfficientNetB2Pscse384X28(nn.Module):\n",
    "    \"\"\" Ensembles ResNet50Pscse_512x28x28 and EfficientNetB2LPscse_384x28x28 \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes: int,\n",
    "                 out_channels: list = None,  # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain: bool = True,\n",
    "                 dropout: float = 0.3,\n",
    "                 activation: str = 'leakyrelu',\n",
    "                 reduction: int = 16,\n",
    "                 separate_inputs: int = None):  # separate_inputs defines the number of inputs\n",
    "\n",
    "        super(EnsembleResNet50_512x28PscseEfficientNetB2Pscse384X28, self).__init__()\n",
    "\n",
    "        self.separate_inputs = separate_inputs\n",
    "\n",
    "        model1 = ResNet50Pscse_512x28x28(num_classes, out_channels, pretrain, dropout, activation, reduction)\n",
    "        model2 = EfficientNetB2LPscse_384x28x28(num_classes, out_channels, pretrain, dropout, activation, reduction)\n",
    "\n",
    "        self.ens_model1 = FeatureEnsemble2models(model1, model2, trim1=1, trim2=1)  # clip classification head\n",
    "\n",
    "        # Create a list of models for separate inputs if separate_inputs is specified\n",
    "        if self.separate_inputs is not None:\n",
    "            self.ensemble_models = nn.ModuleList([deepcopy(self.ens_model1) for _ in range(self.separate_inputs)])\n",
    "\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.separate_inputs is not None:\n",
    "\n",
    "            # Ensure input data has self.separate_inputs no. of channels\n",
    "            if x.shape[1] < self.separate_inputs:\n",
    "                raise ValueError(f\"Can't split. Input data has {x.shape[1]} channels whereas separate_inputs parameter is {self.separate_inputs}. \\\n",
    "Check the separate_inputs parameter in the config file.\")\n",
    "            \n",
    "            features_list = []\n",
    "\n",
    "            # Loop over each input channel, process it, and store the features\n",
    "            for i in range(self.separate_inputs):\n",
    "                # Separate the i-th input (single channel)\n",
    "                xi = x[:, i:i + 1, :, :]  # extract ith channel\n",
    "\n",
    "                # Convert to 3 channels by repeating or concatenating along the channel dimension\n",
    "                xi_3ch = torch.cat([xi, xi, xi], dim=1)\n",
    "\n",
    "                # Get features from the i-th ensemble model\n",
    "                features_i = self.ensemble_models[i](xi_3ch)\n",
    "\n",
    "                # Collect features\n",
    "                features_list.append(features_i)\n",
    "\n",
    "            # Concatenate features along the channel dimension\n",
    "            features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        else:\n",
    "            features = self.ens_model1(x)\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211200a-e0d1-4f83-8941-e4dfe02bdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnsembleResNet50_512x28PscseEfficientNetB2Pscse384X28\n",
    "if __name__ == \"__main__\":\n",
    "    inp=torch.rand(1, 3, 224, 224)\n",
    "    num_classes=2\n",
    "    out_channels=[6144, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout=0.3\n",
    "    activation='leakyrelu'\n",
    "    reduction=16\n",
    "    separate_inputs = 3\n",
    "    \n",
    "    model = EnsembleResNet50_512x28PscseEfficientNetB2Pscse384X28(num_classes, out_channels, pretrain, dropout, activation, reduction, separate_inputs)\n",
    "    \n",
    "    out = model(inp)\n",
    "    \n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbaaa90-cf6e-4244-a22e-1c75d4ae5726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b9503-1e81-41df-a93a-20935009fc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15b30f1-82de-44de-a5bd-c9d0cc17c8f6",
   "metadata": {},
   "source": [
    "### EnsembleResNet18Ft512_MBV3LFt960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dcfde00-d69a-45a1-bc74-b8a2cecedc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleResNet18Ft512_MBV3LFt960(nn.Module):\n",
    "    \"\"\" Ensembles ResNet18 with 512 features and MobileNetV3Large with 960 features \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes: int,\n",
    "                 out_channels: list = None,  # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain: bool = True,\n",
    "                 dropout: float = 0.3,\n",
    "                 in_chs: int = None,\n",
    "                 cls_activation: str = 'leakyrelu',\n",
    "                 separate_inputs: int = None):  # separate_inputs defines the number of inputs\n",
    "\n",
    "        super(EnsembleResNet18Ft512_MBV3LFt960, self).__init__()\n",
    "\n",
    "        self.separate_inputs = separate_inputs\n",
    "\n",
    "        model1 = base_models('resnet18', pretrain=pretrain, num_classes=num_classes, in_chs=in_chs)\n",
    "        model2 = base_models('mobilenet_v3_large', pretrain=pretrain, num_classes=num_classes, in_chs=in_chs)\n",
    "\n",
    "        self.ens_model1 = FeatureEnsemble2models(model1, model2, trim1=2, trim2=2)  # clip classification head\n",
    "\n",
    "        # Create a list of models for separate inputs if separate_inputs is specified\n",
    "        if self.separate_inputs is not None:\n",
    "            self.ensemble_models = nn.ModuleList([deepcopy(self.ens_model1) for _ in range(self.separate_inputs)])\n",
    "\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 activation=cls_activation,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.separate_inputs is not None:\n",
    "\n",
    "            # Ensure input data has self.separate_inputs no. of channels\n",
    "            if x.shape[1] < self.separate_inputs:\n",
    "                raise ValueError(f\"Can't split. Input data has {x.shape[1]} channels whereas separate_inputs parameter is {self.separate_inputs}. \\\n",
    "Check the separate_inputs parameter in the config file.\")\n",
    "            \n",
    "            features_list = []\n",
    "\n",
    "            # Loop over each input channel, process it, and store the features\n",
    "            for i in range(self.separate_inputs):\n",
    "                # Separate the i-th input (single channel)\n",
    "                xi = x[:, i:i + 1, :, :]  # extract ith channel\n",
    "\n",
    "                # Convert to 3 channels by repeating or concatenating along the channel dimension\n",
    "                xi_3ch = torch.cat([xi, xi, xi], dim=1)\n",
    "\n",
    "                # Get features from the i-th ensemble model\n",
    "                features_i = self.ensemble_models[i](xi_3ch)\n",
    "\n",
    "                # Collect features\n",
    "                features_list.append(features_i)\n",
    "\n",
    "            # Concatenate features along the channel dimension\n",
    "            features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        else:\n",
    "            features = self.ens_model1(x)\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7392299c-8576-457f-96a7-55be45619e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    inp=torch.rand(1, 3, 224, 224)\n",
    "    num_classes=2\n",
    "    out_channels=[4416, 1024, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout=0.3\n",
    "    in_chs=3\n",
    "    cls_activation='leakyrelu'\n",
    "    separate_inputs = 3\n",
    "\n",
    "\n",
    "    model = EnsembleResNet18Ft512_MBV3LFt960(num_classes, out_channels, pretrain, dropout, in_chs, cls_activation, separate_inputs)\n",
    "    \n",
    "    out = model(inp)\n",
    "    \n",
    "    print(out.shape) # torch.Size([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b4c02-553d-4676-8fad-c12ddd35ed80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7d2f6-0de1-4bd8-ae9a-9261f0901bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16bebe7e-de25-4c9a-b514-b04df3a4683e",
   "metadata": {},
   "source": [
    "### EnsembleEfficientNetB2SFt1408_MBV3LFt960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cd680a3-4c1d-4dce-a981-561e00e1a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleEfficientNetB2SFt1408_MBV3LFt960(nn.Module):\n",
    "    \"\"\" Ensembles EfficientNetB2 with 1408 features and MobileNetV3Large with 960 features \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes: int,\n",
    "                 out_channels: list = None,  # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain: bool = True,\n",
    "                 dropout: float = 0.3,\n",
    "                 in_chs: int = None,\n",
    "                 cls_activation: str = 'leakyrelu',\n",
    "                 separate_inputs: int = None):  # separate_inputs defines the number of inputs\n",
    "\n",
    "        super(EnsembleEfficientNetB2SFt1408_MBV3LFt960, self).__init__()\n",
    "\n",
    "        self.separate_inputs = separate_inputs\n",
    "\n",
    "        model1 = base_models('efficientnet_v2_s', pretrain=pretrain, num_classes=num_classes, in_chs=in_chs)\n",
    "        model2 = base_models('mobilenet_v3_large', pretrain=pretrain, num_classes=num_classes, in_chs=in_chs)\n",
    "\n",
    "        self.ens_model1 = FeatureEnsemble2models(model1, model2, trim1=2, trim2=2)  # clip classification head\n",
    "\n",
    "        # Create a list of models for separate inputs if separate_inputs is specified\n",
    "        if self.separate_inputs is not None:\n",
    "            self.ensemble_models = nn.ModuleList([deepcopy(self.ens_model1) for _ in range(self.separate_inputs)])\n",
    "\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 activation=cls_activation,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.separate_inputs is not None:\n",
    "\n",
    "            # Ensure input data has self.separate_inputs no. of channels\n",
    "            if x.shape[1] < self.separate_inputs:\n",
    "                raise ValueError(f\"Can't split. Input data has {x.shape[1]} channels whereas separate_inputs parameter is {self.separate_inputs}. \\\n",
    "Check the separate_inputs parameter in the config file.\")\n",
    "            \n",
    "            features_list = []\n",
    "\n",
    "            # Loop over each input channel, process it, and store the features\n",
    "            for i in range(self.separate_inputs):\n",
    "                # Separate the i-th input (single channel)\n",
    "                xi = x[:, i:i + 1, :, :]  # extract ith channel\n",
    "\n",
    "                # Convert to 3 channels by repeating or concatenating along the channel dimension\n",
    "                xi_3ch = torch.cat([xi, xi, xi], dim=1)\n",
    "\n",
    "                # Get features from the i-th ensemble model\n",
    "                features_i = self.ensemble_models[i](xi_3ch)\n",
    "\n",
    "                # Collect features\n",
    "                features_list.append(features_i)\n",
    "\n",
    "            # Concatenate features along the channel dimension\n",
    "            features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        else:\n",
    "            features = self.ens_model1(x)\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85166720-e481-4a79-80e7-9c4c1ac06c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    inp=torch.rand(1, 3, 224, 224)\n",
    "    num_classes=2\n",
    "    out_channels=[6720, 1024, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout=0.3\n",
    "    in_chs=3\n",
    "    cls_activation='leakyrelu'\n",
    "    separate_inputs = 3\n",
    "\n",
    "\n",
    "    model = EnsembleEfficientNetB2SFt1408_MBV3LFt960(num_classes, out_channels, pretrain, dropout, in_chs, cls_activation, separate_inputs)\n",
    "    \n",
    "    out = model(inp)\n",
    "    \n",
    "    print(out.shape) # torch.Size([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8816935-4ab9-4355-a201-494a854b3a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add54748-c77b-43fc-961b-d756327a6fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f638399f-cfa5-4176-9a03-da1d3898e1db",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962206f8-f528-48c4-b1a3-e2ebf4f22a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoPlusOneEnsemble(nn.Module):\n",
    "    \"\"\" It takes three models as input. The first two models are ensembled and handle each channel \n",
    "        of the input image individually. The third model handles all channels together. \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 names: list, # e.g. [\"resnet18\", \"efficientnet_v2_s\", \"mobilenet_v3_large\"]\n",
    "                 num_classes: int,\n",
    "                 out_channels: list = None,  # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain: bool = True,\n",
    "                 dropout: float = 0.3,\n",
    "                 in_chs: int = None,\n",
    "                 cls_activation: str = 'leakyrelu',\n",
    "                 separate_inputs: int = None):  # separate_inputs defines the number of inputs\n",
    "\n",
    "        super(EnsembleResNet18Ft512_EfficientNetB2SFt1408, self).__init__()\n",
    "\n",
    "        self.separate_inputs = separate_inputs\n",
    "\n",
    "        model1 = base_models_features_only(names[0], pretrain, num_classes, in_chs)\n",
    "        model2 = base_models_features_only(names[1], pretrain, num_classes, in_chs)\n",
    "        self.model3 = base_models_features_only(names[2], pretrain, num_classes, in_chs)\n",
    "        \n",
    "        self.ens_model1 = FeatureEnsemble2models([model1, model2])  # clip classification head\n",
    "\n",
    "        # Create a list of models for separate inputs if separate_inputs is specified\n",
    "        if self.separate_inputs is not None:\n",
    "            self.ensemble_models = nn.ModuleList([deepcopy(self.ens_model1) for _ in range(self.separate_inputs)])\n",
    "\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 activation=cls_activation,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.separate_inputs is not None:\n",
    "\n",
    "            # Ensure input data has self.separate_inputs no. of channels\n",
    "            if x.shape[1] < self.separate_inputs:\n",
    "                raise ValueError(f\"Can't split. Input data has {x.shape[1]} channels whereas separate_inputs parameter is {self.separate_inputs}. \\\n",
    "Check the separate_inputs parameter in the config file.\")\n",
    "            \n",
    "            features_list = []\n",
    "\n",
    "            # Loop over each input channel, process it, and store the features\n",
    "            for i in range(self.separate_inputs):\n",
    "                # Separate the i-th input (single channel)\n",
    "                xi = x[:, i:i + 1, :, :]  # extract ith channel\n",
    "\n",
    "                # Convert to 3 channels by repeating or concatenating along the channel dimension\n",
    "                xi_3ch = torch.cat([xi, xi, xi], dim=1)\n",
    "\n",
    "                # Get features from the i-th ensemble model\n",
    "                features_i = self.ensemble_models[i](xi_3ch)\n",
    "\n",
    "                # Collect features\n",
    "                features_list.append(features_i)\n",
    "\n",
    "            # Train the model using all channels\n",
    "            features_all_chs = self.model3(x)\n",
    "            features_list.append(features_all_chs)\n",
    "            \n",
    "            # Concatenate features along the channel dimension\n",
    "            features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        else:\n",
    "            features = self.ens_model1(x)\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f1938-1f86-4a9f-86b9-bcce7516023d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcb7f3-4f7e-4cae-9d73-7ad3ca13bb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ead6914-9518-43c2-9a92-d6ec00e2910e",
   "metadata": {},
   "source": [
    "### BaseModelSepIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ffaf5-679e-44e0-8229-ae95cc0201be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModelSepIn(nn.Module):\n",
    "    \"\"\"This model takes a base model and creates multiple copies based on the number of channels in the input image. \n",
    "    During training, it first splits all channels and uses each copy to train on each split. Finally, it concatenates \n",
    "    all channels and sends them to the classification head.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, \n",
    "                 num_classes:int=None, \n",
    "                 out_channels:list=None, # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain:bool=True, \n",
    "                 dropout: float = 0.3,\n",
    "                 in_chs:int=None,\n",
    "                 cls_activation: str = 'leakyrelu', # activation in classification head\n",
    "                 separate_inputs:int=None): # separate_inputs defines the number of inputs\n",
    "        \n",
    "        super(BaseModelSepIn, self).__init__()\n",
    "\n",
    "        self.separate_inputs = separate_inputs\n",
    "\n",
    "        # Load the base model\n",
    "        base_model = base_models_features_only(name, pretrain, num_classes, in_chs)\n",
    "        \n",
    "        # Create a list of models for separate inputs \n",
    "        self.ensemble_models = nn.ModuleList([deepcopy(base_model) for _ in range(self.separate_inputs)])\n",
    "\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 activation=cls_activation,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.separate_inputs is not None:\n",
    "\n",
    "            # Ensure input data has self.separate_inputs no. of channels\n",
    "            if x.shape[1] < self.separate_inputs:\n",
    "                raise ValueError(f\"Can't split. Input data has {x.shape[1]} channels whereas separate_inputs parameter is {self.separate_inputs}. \\\n",
    "Check the separate_inputs parameter in the config file.\")\n",
    "            \n",
    "            features_list = []\n",
    "\n",
    "            # Loop over each input channel, process it, and store the features\n",
    "            for i in range(self.separate_inputs):\n",
    "                # Separate the i-th input (single channel)\n",
    "                xi = x[:, i:i + 1, :, :]  # extract ith channel\n",
    "\n",
    "                # Convert to 3 channels by repeating or concatenating along the channel dimension\n",
    "                xi_3ch = torch.cat([xi, xi, xi], dim=1)\n",
    "\n",
    "                # Get features from the i-th ensemble model\n",
    "                features_i = self.ensemble_models[i](xi_3ch)\n",
    "\n",
    "                # Collect featureas\n",
    "                features_list.append(features_i)\n",
    "\n",
    "            # Concatenate features along the channel dimension\n",
    "            features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        else:\n",
    "            # features = self.ens_model1(x)\n",
    "            raise ValueError(\"Input parameter `separate_inputs` can't be None. Check the config file.\")\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa8955-e2b5-43f0-867d-b5fd549f918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    name = 'convnext_large'\n",
    "    inp=torch.rand(1, 3, 224, 224)\n",
    "    num_classes=2\n",
    "    in_chs=3\n",
    "    out_channels=[4608, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout=0.3\n",
    "    activation='leakyrelu'\n",
    "    separate_inputs = 3\n",
    "    \n",
    "    model = BaseModelSepIn(name, num_classes, out_channels, pretrain, dropout, in_chs, activation, separate_inputs)\n",
    "    \n",
    "    out = model(inp)\n",
    "    \n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a880e9-5435-497c-8b5e-5ee2bae9c61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf950e72-8428-4c5d-8127-615fb367a600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1702feb-02d0-4ba6-b28c-ce3d300c54bb",
   "metadata": {},
   "source": [
    "### EnsembleModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034853c-4043-4978-9a1f-06be4b81c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModels(nn.Module):\n",
    "    \"\"\" Ensembles models available in base_models_features_collection.py\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 names: list, # e.g. ['resnet18', 'efficientnet_v2_s', 'mobilenet_v3_large'] \n",
    "                 num_classes: int,\n",
    "                 out_channels: list = None, # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain: bool = True,\n",
    "                 dropout: float = 0.3,\n",
    "                 in_chs: int = None,\n",
    "                 cls_activation: str = 'leakyrelu', # activation in classification head \n",
    "                 input_seq: list = [[0,0,0], [1,1,1], [2,2,2], [0,1,2]],\n",
    "                 # input_seq is a nested list. Consider, input_seq = [[0,0,0], [1,1,1], [2,2,2], [0,1,2]]\n",
    "                 # It means that it expects four copies of the ensemble model. 0,1,2 are channel indices of \n",
    "                 # the original input image coming from the dataloader. So, the 1st ensemble model will \n",
    "                 # take the input image consisting of 3 copies of the 0th-ch of the original image. Similarly,\n",
    "                 # 2nd and 3rd ensemble models takes 3 copies of 1st- and 2nd-ch of the original image, respectively.\n",
    "                 # The 4th ensemble model takes input image that consists of 0-th, 1st-, and 2nd-ch of the \n",
    "                 # original image. \n",
    "                \n",
    "                ):  \n",
    "\n",
    "        super(EnsembleModels, self).__init__()\n",
    "\n",
    "        self.input_seq = input_seq\n",
    "\n",
    "        # Load models\n",
    "        models = [base_models_features_only(name, pretrain, num_classes, in_chs) for name in names] \n",
    "\n",
    "        self.ens_model1 = FeatureEnsembleNModelsNoTrim(models)  # ensemble features\n",
    "\n",
    "        # Create a list of models if length of input_seq is greater than 1\n",
    "        self.num_copies = len(self.input_seq) # similar to separate_inputs\n",
    "        if self.num_copies > 1:\n",
    "            self.ensemble_models = nn.ModuleList([deepcopy(self.ens_model1) for _ in range(self.num_copies)])\n",
    "\n",
    "        # Classification head\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 activation=cls_activation,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.num_copies > 1: # multiple copies of ensemble model \n",
    "            \n",
    "            features_list = []\n",
    "\n",
    "            # Loop over each input channel, process it, and store the features\n",
    "            for i in range(self.num_copies):\n",
    "                # Create input image for the i-th ensemble model based on channel indices indicated by input_seq[i]\n",
    "                xi = x[:, self.input_seq[i], :, :]\n",
    "\n",
    "                # Get features from the i-th ensemble model\n",
    "                features_i = self.ensemble_models[i](xi)\n",
    "\n",
    "                # Collect features\n",
    "                features_list.append(features_i)\n",
    "\n",
    "            # Concatenate features along the channel dimension\n",
    "            features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        else:\n",
    "            features = self.ens_model1(x)\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142e741-ef9b-459a-8030-164437d442ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    inp=torch.rand(1, 3, 224, 224)\n",
    "    \n",
    "    names = ['resnet18', 'efficientnet_v2_s', 'mobilenet_v3_large'] \n",
    "    num_classes=2\n",
    "    out_channels=[11008, 4608, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout=0.3\n",
    "    in_chs=3\n",
    "    cls_activation='leakyrelu'\n",
    "    input_seq: list = [[0,0,0], [1,1,1], [2,2,2], [0,1,2]]\n",
    "    \n",
    "    model = EnsembleModels(names, num_classes, out_channels, pretrain, dropout, in_chs, cls_activation, input_seq)\n",
    "    \n",
    "    out = model(inp)\n",
    "    \n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c4e4e-0d48-43cf-a867-b773efae4f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9821f-bbb9-485c-ba1d-021ed3332b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f354497-1841-47cb-9d3e-523d7acf1ebd",
   "metadata": {},
   "source": [
    "### EnsembleModelsV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8367362e-2c7f-4825-911f-c061dff61f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModelsV2(nn.Module):\n",
    "    \"\"\" Ensembles models available in base_models_features_collection.py.\n",
    "        V2 offers more flexibility than V1. In V2, we can create single models, ensemble models, even multiple\n",
    "        ensemble models. We also can control which image will go to which model. For details, read the description\n",
    "        for input_seq and preensemble below.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 names: list, # e.g. ['resnet18', 'efficientnet_v2_s', 'mobilenet_v3_large'] \n",
    "                 num_classes: int,\n",
    "                 out_channels: list = None, # for instance [1024, 512, 256]. Used in classification head\n",
    "                 pretrain: bool = True, # used in base_models_features_only\n",
    "                 dropout: float = 0.3, # used in classification head\n",
    "                 in_chs: int = 3, # used in base_models_features_only\n",
    "                 cls_activation: str = 'leakyrelu', # activation in classification head \n",
    "                 input_seq: list = None, # e.g. [[0,0,0], [1,1,1], [2,2,2], [0,1,2]],\n",
    "                 preensemble: list = None, # e.g. [[0], [1], [0,1], [2]],\n",
    "                 \n",
    "                 # input_seq is a nested list. Consider, input_seq = [[0,0,0], [1,1,1], [2,2,2], [0,1,2]]\n",
    "                 # It means that it expects four copies of the ensemble model. 0,1,2 are channel indices of \n",
    "                 # the original input image coming from the dataloader. So, the 1st ensemble model will \n",
    "                 # take the input image consisting of 3 copies of the 0th-ch of the original image. Similarly,\n",
    "                 # 2nd and 3rd ensemble models takes 3 copies of 1st- and 2nd-ch of the original image, respectively.\n",
    "                 # The 4th ensemble model takes input image that consists of 0-th, 1st-, and 2nd-ch of the \n",
    "                 # original image. \n",
    "\n",
    "                 # preensemble is a nested list. Consider, model names are ['resnet18', 'efficientnet_v2_s', 'mobilenet_v3_large'], \n",
    "                 # and input_seq is [[0,0,0], [1,1,1], [2,2,2], [0,1,2]]. If the preensemble is [[0], [1], [0,1], [2]],\n",
    "                 # then for [0], it will call resnet18 and pass the image [0,0,0] to it. For [1], image [1,1,1] will be fed to \n",
    "                 # efficientnet_v2_s. For [0,1], it will ensemble resnet18 and efficientnet_v2_s, and then image [2,2,2] will\n",
    "                 # be fed to the ensembled model. For [2], image [0,1,2] will be fed to mobilenet_v3_large. \n",
    "                \n",
    "                ):  \n",
    "\n",
    "        super(EnsembleModelsV2, self).__init__()\n",
    "\n",
    "        self.input_seq = input_seq\n",
    "\n",
    "        # Load models\n",
    "        models_ = [base_models_features_only(name, pretrain, num_classes, in_chs) for name in names]\n",
    "\n",
    "        if preensemble is not None:\n",
    "            self.models = []\n",
    "            for ind in preensemble:\n",
    "                if len(ind) == 1: selected_models = [models_[ind[0]]] # handle single model case explicitly     \n",
    "                else: selected_models = list(itemgetter(*ind)(models_)) # handle multiple model selection using itemgetter\n",
    "                    \n",
    "                # Pass the selected models to FeatureEnsembleNModelsNoTrim\n",
    "                self.models.append(FeatureEnsembleNModelsNoTrim(selected_models))\n",
    "        else:\n",
    "            self.models = models_\n",
    "\n",
    "        self.models = nn.ModuleList(self.models)\n",
    "             \n",
    "        # Classification head\n",
    "        self.classification = ClassificationHead(num_classes=num_classes,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 activation=cls_activation,\n",
    "                                                 dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(self.input_seq) != len(self.models):\n",
    "            raise ValueError(f\"Length of input_seq ({len(self.input_seq)}) is not the same as the number of models ({len(self.models)}).\\\n",
    "            They should have the same length.\")\n",
    "\n",
    "        features_list = []\n",
    "        for i in range(len(self.models)):\n",
    "            # Create input image for the i-th model based on channel indices indicated by input_seq[i]\n",
    "            xi = x[:, self.input_seq[i], :, :]\n",
    "\n",
    "            # Get features from the i-th model\n",
    "            features_i = self.models[i](xi)\n",
    "\n",
    "            # Collect features\n",
    "            features_list.append(features_i)\n",
    "\n",
    "        # Concatenate features along the channel dimension\n",
    "        features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        # Pass the features through the classification head\n",
    "        out = self.classification(features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b79da8d-872e-4abf-8505-4760ba402c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    inp=torch.rand(1, 3, 224, 224)\n",
    "    \n",
    "    names = ['resnet18', 'efficientnet_v2_s', 'mobilenet_v3_large'] \n",
    "    num_classes=2\n",
    "    out_channels=[3264, 1024, 512, 256]\n",
    "    pretrain = True\n",
    "    dropout=0.3\n",
    "    in_chs=3\n",
    "    cls_activation='leakyrelu'\n",
    "    input_seq = [[0,0,0], [1,1,1], [0,1,2]]\n",
    "    preensemble = [[0], [0,1], [2]]\n",
    "    \n",
    "    \n",
    "    model = EnsembleModelsV2(names, num_classes, out_channels, pretrain, dropout, in_chs, cls_activation, input_seq, preensemble)\n",
    "    \n",
    "    out = model(inp)\n",
    "    \n",
    "    print(out.shape) # torch.Size([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de840dbb-34c8-4bbf-9f5e-5a5147994e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6563a5-8656-4760-b1b9-17d5186452e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2e306-69ed-470a-8183-7a82e9b5ad77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a7881-0f6f-42b5-8492-300c2aa5e8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649ffc9-b96a-4fca-b29d-22755eb5fe01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
